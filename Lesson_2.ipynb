{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJx-7ybmnzuy"
      },
      "source": [
        "# Домашнее задание к уроку 2. Создание признакового пространства\n",
        "\n",
        "все материалы для выполения дз в `sem2.ipynb`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq88rVQynzu1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6dTBi-qnzu2",
        "outputId": "c5050e08-1c96-4d5b-aa6c-709d59ec4055"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text     label\n",
              "0  @first_timee хоть я и школота, но поверь, у на...  positive\n",
              "1  Да, все-таки он немного похож на него. Но мой ...  positive\n",
              "2  RT @KatiaCheh: Ну ты идиотка) я испугалась за ...  positive\n",
              "3  RT @digger2912: \"Кто то в углу сидит и погибае...  positive\n",
              "4  @irina_dyshkant Вот что значит страшилка :D\\nН...  positive"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# считываем данные и заполняем общий датасет\n",
        "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
        "positive['label'] = ['positive'] * len(positive)\n",
        "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
        "negative['label'] = ['negative'] * len(negative)\n",
        "df = positive.append(negative)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Us6ImIBnzu3",
        "outputId": "f57776e5-5fc1-4e05-a17e-19440b2e97f5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>111918</th>\n",
              "      <td>Но не каждый хочет что то исправлять:( http://...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111919</th>\n",
              "      <td>скучаю так :-( только @taaannyaaa вправляет мо...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111920</th>\n",
              "      <td>Вот и в школу, в говно это идти уже надо(</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111921</th>\n",
              "      <td>RT @_Them__: @LisaBeroud Тауриэль, не грусти :...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111922</th>\n",
              "      <td>Такси везет меня на работу. Раздумываю приплат...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text     label\n",
              "111918  Но не каждый хочет что то исправлять:( http://...  negative\n",
              "111919  скучаю так :-( только @taaannyaaa вправляет мо...  negative\n",
              "111920          Вот и в школу, в говно это идти уже надо(  negative\n",
              "111921  RT @_Them__: @LisaBeroud Тауриэль, не грусти :...  negative\n",
              "111922  Такси везет меня на работу. Раздумываю приплат...  negative"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF7SqhwWnzu4"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-I1xZUqnzu4"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression # можно заменить на любимый классификатор\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNuJkqX1nzu4",
        "outputId": "f4c6dca8-0489-4019-c540-7e5094407ed1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package genesis to\n",
            "[nltk_data]     /Users/alenakukhta/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/alenakukhta/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk import ngrams\n",
        "import nltk\n",
        "from nltk import collocations \n",
        "nltk.download('genesis')\n",
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.words('russian'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFEYu5clnzu5"
      },
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "noise = stopwords.words('russian') + list(punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoPOzH3tnzu5"
      },
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlkREeTPnzu5",
        "outputId": "bc475fe5-5a76-4a54-b5d4-8f356cd88fdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2870536\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['first_timee', 'хоть', 'я', 'и', 'школота', 'но', 'поверь', 'у', 'нас', 'то']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus = [token for tweet in df.text for token in word_tokenize(tweet) if token not in punctuation]\n",
        "print(len(corpus))\n",
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qbu_Sjg4nzu6",
        "outputId": "40036eb7-d8a0-42ca-8594-aac58b1d8b0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('не', 69472),\n",
              " ('и', 55166),\n",
              " ('в', 52902),\n",
              " ('я', 52818),\n",
              " ('RT', 38070),\n",
              " ('на', 35759),\n",
              " ('http', 32998),\n",
              " ('что', 31541),\n",
              " ('с', 27217),\n",
              " ('а', 26860)]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "freq_dict = Counter(corpus)\n",
        "freq_dict_sorted= sorted(freq_dict.items(), key=lambda x: -x[1])\n",
        "list(freq_dict_sorted)[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw0GieJSMU-O"
      },
      "source": [
        "## Задание 1.\n",
        "\n",
        "**Задание**: обучите три классификатора: \n",
        "\n",
        "1) на токенах с высокой частотой \n",
        "\n",
        "2) на токенах со средней частотой \n",
        "\n",
        "3) на токенах с низкой частотой\n",
        "\n",
        "Сравните полученные результаты, оцените какие токены наиболее важные для классификации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUQ6kAgPMqNn",
        "outputId": "27969cfc-e685-4e29-cdbc-f32e38c284a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29 507 350565\n"
          ]
        }
      ],
      "source": [
        "# Подготовим три группы токенов: с высокой, средней и низкой частотой\n",
        "\n",
        "high_tokens = set()\n",
        "med_tokens = set()\n",
        "lit_tokens = set()\n",
        "h = 10000 # если частота выше этого значения будем считать что это токен с высокой частотой\n",
        "m = 500 # если частота ниже этого значения будем считать что это токен с низкой частотой\n",
        "for i in freq_dict_sorted:\n",
        "    if i[1] > h:\n",
        "        high_tokens.add(i[0])\n",
        "    elif i[1] < m:\n",
        "        lit_tokens.add(i[0])\n",
        "    else:\n",
        "        med_tokens.add(i[0])\n",
        "print(len(high_tokens),len(med_tokens),len(lit_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfTbptIRnzu7"
      },
      "outputs": [],
      "source": [
        "# создадим фильтр (стопслова) с добавлением токенов средней и низкой частоты, \n",
        "# то есть будем считать только по токенам с высокой частотой\n",
        "sw = noise + list(lit_tokens) + list(med_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgTJW_alnzu7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKnEuQjlnzu8",
        "outputId": "6a8da8c3-6796-4a52-f92b-1bc52e4bd7eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.80      0.57      0.67     38962\n",
            "    positive       0.42      0.68      0.52     17747\n",
            "\n",
            "    accuracy                           0.61     56709\n",
            "   macro avg       0.61      0.63      0.59     56709\n",
            "weighted avg       0.68      0.61      0.62     56709\n",
            "\n",
            "CPU times: user 1min, sys: 178 ms, total: 1min\n",
            "Wall time: 55.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=sw)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCXq-64gnzu8"
      },
      "outputs": [],
      "source": [
        "# посчитаем только по токенам с средней частотой\n",
        "sw = noise + list(high_tokens) + list(lit_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8n0ucoQnzu8",
        "outputId": "8b7e4e31-6b15-406c-b541-2486cd7f5bdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.58      0.68      0.63     23731\n",
            "    positive       0.74      0.64      0.68     32978\n",
            "\n",
            "    accuracy                           0.66     56709\n",
            "   macro avg       0.66      0.66      0.65     56709\n",
            "weighted avg       0.67      0.66      0.66     56709\n",
            "\n",
            "CPU times: user 1min 2s, sys: 291 ms, total: 1min 3s\n",
            "Wall time: 57.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=sw)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xwzv_N9cnzu9"
      },
      "outputs": [],
      "source": [
        "# посчитаем только по токенам с низкой частотой\n",
        "sw = noise + list(high_tokens) + list(med_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aJQfm0Enzu9",
        "outputId": "d6512c73-8669-4576-a2cb-5035a028aa9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.79      0.72      0.76     30895\n",
            "    positive       0.70      0.78      0.74     25814\n",
            "\n",
            "    accuracy                           0.75     56709\n",
            "   macro avg       0.75      0.75      0.75     56709\n",
            "weighted avg       0.75      0.75      0.75     56709\n",
            "\n",
            "CPU times: user 49.8 s, sys: 272 ms, total: 50.1 s\n",
            "Wall time: 39.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=sw)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di5nEjN0nzu-"
      },
      "source": [
        "**Вывод:** Лучшая точность получилась на токенах с низкой частотностью, имеется зависимость от смещения границ разделения частотности, если за границу брать количественное значение частотности, лучший результат дает низкая частотность."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV3fmzp-LVSU"
      },
      "source": [
        "## О важности эксплоративного анализа\n",
        "\n",
        "Но иногда пунктуация бывает и не шумом -- главное отталкиваться от задачи. Что будет если вообще не убирать пунктуацию?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjkMxK9VLVSV",
        "outputId": "dfea56d5-4d92-4862-9788-29c8c8db29ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00     27997\n",
            "    positive       1.00      1.00      1.00     28712\n",
            "\n",
            "    accuracy                           1.00     56709\n",
            "   macro avg       1.00      1.00      1.00     56709\n",
            "weighted avg       1.00      1.00      1.00     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2fRbUAvLVSX"
      },
      "source": [
        "Шок! Стоило оставить пунктуацию -- и все метрики равны 1. Как это получилось? Среди неё были очень значимые токены (как вы думаете, какие?). Найдите фичи с самыми большими коэффициэнтами:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGGmiKvinzu_"
      },
      "source": [
        "## Задание 2.\n",
        "\n",
        "найти фичи с наибольшей значимостью, и вывести их"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3TuktZcnzu_"
      },
      "outputs": [],
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_JRuyuRLVSY",
        "outputId": "c5826f72-1253-43f5-af0a-10bf561c7ba1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00     28001\n",
            "    positive       1.00      1.00      1.00     28708\n",
            "\n",
            "    accuracy                           1.00     56709\n",
            "   macro avg       1.00      1.00      1.00     56709\n",
            "weighted avg       1.00      1.00      1.00     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Оставим знаки пунктуации  \n",
        "vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=stopwords.words('russian'))\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbJX_a2fnzvA",
        "outputId": "2b42e81e-53e4-4034-cbc1-8d4352a103ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# существенное влияние оказывают симолы, участвующие в смайлах:\n",
        "punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQzgU-vAnzvA",
        "outputId": "c09b58e8-3d37-4976-fe3c-d2aa6abc1837"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\"',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " '&',\n",
              " \"'\",\n",
              " '+',\n",
              " ',',\n",
              " '.',\n",
              " '<',\n",
              " '=',\n",
              " '>',\n",
              " '?',\n",
              " '@',\n",
              " '[',\n",
              " '\\\\',\n",
              " ']',\n",
              " '^',\n",
              " '`',\n",
              " '{',\n",
              " '|',\n",
              " '}',\n",
              " '~']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p = list('\"#$%&\\'+,.<=>?@[\\\\]^`{|}~')\n",
        "p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBwhvrB1nzvA",
        "outputId": "e239feff-d858-4ca6-809d-85500101aae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00     27984\n",
            "    positive       1.00      1.00      1.00     28725\n",
            "\n",
            "    accuracy                           1.00     56709\n",
            "   macro avg       1.00      1.00      1.00     56709\n",
            "weighted avg       1.00      1.00      1.00     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "myfilter = stopwords.words('russian') + p\n",
        "vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words= myfilter)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vtAyItvLVSb"
      },
      "source": [
        "Посмотрим, как один из супер-значительных токенов справится с классификацией безо всякого машинного обучения:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqH07o-7LVSc",
        "outputId": "fad0a24a-98ee-4f84-8782-495548eb0fb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.85      0.92     32923\n",
            "    positive       0.83      1.00      0.91     23786\n",
            "\n",
            "    accuracy                           0.91     56709\n",
            "   macro avg       0.92      0.93      0.91     56709\n",
            "weighted avg       0.93      0.91      0.91     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cool_token = ')'\n",
        "pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVBOTO8PnzvB",
        "outputId": "9a44c34c-ff67-4ffb-8eeb-017550bc6916"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('!', 0.5162672591652119), ('\"', 0.5074150487576928), ('#', 0.5047875998518754), ('$', 0.49487735632791974), ('%', 0.4974342696926414), ('&', 0.4943659736549754), (\"'\", 0.49477155301627607), ('(', 0.02572783861468197), (')', 0.9142464159128181), ('*', 0.5137985152268599), ('+', 0.4955827117388774), (',', 0.5063393817559823), ('-', 0.5114708423706995), ('.', 0.5101835687457017), ('/', 0.5434058086018092), (':', 0.5480787881994039), (';', 0.4994798003844187), ('<', 0.494806820786824), ('=', 0.4944188753107972), ('>', 0.494806820786824), ('?', 0.5034297906857818), ('@', 0.566753072704509), ('[', 0.4949478918690155), ('\\\\', 0.49484208855737183), (']', 0.4950713290659331), ('^', 0.5005907351566771), ('_', 0.5183480576275371), ('`', 0.49463048193408454), ('{', 0.494806820786824), ('|', 0.49034544781251654), ('}', 0.494806820786824), ('~', 0.49489499021319366)]\n"
          ]
        }
      ],
      "source": [
        "pp = []\n",
        "for i in punctuation:\n",
        "    cool_token = i\n",
        "    pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
        "    pp.append((i,accuracy_score(pred, y_test)))\n",
        "print(pp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7V8zX6OnzvC"
      },
      "source": [
        "Выведем фичи с наибольшей значимостью"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8YH_C1knzvC",
        "outputId": "f55a4583-ef59-4d74-d072-be4a9e6ba693"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ")   -    0.9142464159128181\n",
            "@   -    0.566753072704509\n",
            ":   -    0.5480787881994039\n",
            "/   -    0.5434058086018092\n",
            "_   -    0.5183480576275371\n",
            "!   -    0.5162672591652119\n",
            "*   -    0.5137985152268599\n",
            "-   -    0.5114708423706995\n",
            ".   -    0.5101835687457017\n",
            "\"   -    0.5074150487576928\n"
          ]
        }
      ],
      "source": [
        "for i in sorted(pp, key=lambda x: -x[1])[:10]:\n",
        "    print(i[0],'  -   ' ,i[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5THCOjMLVSg"
      },
      "source": [
        "## Символьные n-граммы\n",
        "\n",
        "Теперь в качестве фичей используем, например, униграммы символов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIUwDOabLVSh",
        "outputId": "54f129b1-994f-448e-e861-1912b4a21cdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.99      1.00      1.00     27946\n",
            "    positive       1.00      0.99      1.00     28763\n",
            "\n",
            "    accuracy                           1.00     56709\n",
            "   macro avg       1.00      1.00      1.00     56709\n",
            "weighted avg       1.00      1.00      1.00     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(analyzer='char', ngram_range=(1, 1))\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_E0uPpgLVSj"
      },
      "source": [
        "В общем-то, теперь уже понятно, почему на этих данных здесь 1. Так или инчае, на символах классифицировать тоже можно: для некторых задач (например, для определения языка) фичи-символьные n-граммы решительно рулят.\n",
        "\n",
        "Ещё одна замечательная особенность фичей-символов: токенизация и лемматизация не нужна, можно использовать такой подход для языков, у которых нет готвых анализаторов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY0u5y18nzvD"
      },
      "source": [
        "## Задание 3.\n",
        "\n",
        "1) сравнить count/tf-idf/hashing векторайзеры/полносвязанную сетку (построить classification_report)\n",
        "\n",
        "2) подобрать оптимальный размер для hashing векторайзера \n",
        "\n",
        "3) убедиться что для сетки нет переобучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwhJMleonzvD",
        "outputId": "6ecd4025-c852-4b9c-f308-79b22bf7db15"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "X has 346 features per sample; expecting 259788",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0m\u001b[1;32m    289\u001b[0m                              % (X.shape[1], n_features))\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 346 features per sample; expecting 259788"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "myfilter = stopwords.words('russian') + p\n",
        "\n",
        "count_vect = CountVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words= myfilter)\n",
        "bow = count_vect.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSu7a6o4nzvD",
        "outputId": "b984f379-5fb9-4193-dade-58076ef3d31a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00     28001\n",
            "    positive       1.00      1.00      1.00     28708\n",
            "\n",
            "    accuracy                           1.00     56709\n",
            "   macro avg       1.00      1.00      1.00     56709\n",
            "weighted avg       1.00      1.00      1.00     56709\n",
            "\n",
            "CPU times: user 48.9 s, sys: 222 ms, total: 49.1 s\n",
            "Wall time: 40.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "myfilter = stopwords.words('russian')\n",
        "tf_vect = TfidfVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words= myfilter)\n",
        "bow = tf_vect.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(tf_vect.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLgl0AgbnzvE"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yktnyYpnzvE",
        "outputId": "5b0cd7fd-df0a-4515-d713-8ee9dc0a87bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.50      0.54      0.52     25851\n",
            "    positive       0.59      0.55      0.57     30858\n",
            "\n",
            "    accuracy                           0.54     56709\n",
            "   macro avg       0.54      0.54      0.54     56709\n",
            "weighted avg       0.55      0.54      0.55     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vectorizer = HashingVectorizer(n_features=2**4,)\n",
        "bow = vectorizer.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vectorizer.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE__Jm1cnzvE",
        "outputId": "e966f9d1-3146-46ae-deaa-c3b1e176e3c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.60      0.61      0.60     27427\n",
            "    positive       0.63      0.61      0.62     29282\n",
            "\n",
            "    accuracy                           0.61     56709\n",
            "   macro avg       0.61      0.61      0.61     56709\n",
            "weighted avg       0.61      0.61      0.61     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vectorizer = HashingVectorizer(n_features=2**8,)\n",
        "bow = vectorizer.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vectorizer.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xT4BarM2nzvE",
        "outputId": "d162b9bd-1a6e-42c9-a7a4-9084a2feb394"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.64      0.65      0.64     27421\n",
            "    positive       0.67      0.65      0.66     29288\n",
            "\n",
            "    accuracy                           0.65     56709\n",
            "   macro avg       0.65      0.65      0.65     56709\n",
            "weighted avg       0.65      0.65      0.65     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vectorizer = HashingVectorizer(n_features=2**10,)\n",
        "bow = vectorizer.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vectorizer.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKgB9in9nzvF",
        "outputId": "c7cea8fd-1311-4efa-cc9b-fde4f4abbf92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.68      0.69      0.68     27422\n",
            "    positive       0.70      0.69      0.70     29287\n",
            "\n",
            "    accuracy                           0.69     56709\n",
            "   macro avg       0.69      0.69      0.69     56709\n",
            "weighted avg       0.69      0.69      0.69     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vectorizer = HashingVectorizer(n_features=2**12,)\n",
        "bow = vectorizer.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vectorizer.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gJABxhalLVQu",
        "IaQMCGHFLVQ6",
        "5AJk1B39LVRP",
        "RJlvqWuALVRs",
        "rck5OVqhLVSA",
        "mV3fmzp-LVSU",
        "H5THCOjMLVSg",
        "02s2Vh7MLVSj",
        "b1khxRFDLVSm",
        "sfUmWcAQLVSt",
        "BxvtN-3zLVS5",
        "gyrHhYkgLVTB"
      ],
      "name": "Lesson_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}